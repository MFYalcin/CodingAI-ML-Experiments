{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pre-Processing Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some data. IMDB data available [here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"file_path_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First glance, define some printing functions for easier viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a random sample from the series\n",
    "def print_first_n(series, n=10):\n",
    "    print(f\"First n sample (n = {n})\", end=\"\\n\\n\")\n",
    "    for i, sent in enumerate(series[:n]):\n",
    "        print(f\"{i+1}) {sent}\")\n",
    "        print()\n",
    "\n",
    "# print a random sample from the series\n",
    "def print_sample(series, n=10):\n",
    "    print(f\"Random sample (n = {n})\", end=\"\\n\\n\")\n",
    "    for i, sent in enumerate(series.sample(n, replace=False)):\n",
    "        print(f\"{i+1}) {sent}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the reviews\n",
    "reviews = imdb_df['review'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at the first 10 reviews\n",
    "print_first_n(reviews, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# remove the random breaks: <br /><br />\n",
    "def remove_random_breaks(s):\n",
    "    return re.sub(\"<br /><br />\", \" \", x)\n",
    "\n",
    "# apply the function to all the reviews\n",
    "reviews = reviews.apply(remove_random_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print a random sample of 5 reviews\n",
    "print_sample(reviews, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tokenize\n",
    "Splitting a string up into words or meaningful/useful parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK (Natural Language ToolKit) is a common package for doing language processing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more info on NLTK, see [their documentation.](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I do not like green eggs and ham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Remove stopwords (and punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's basic stopwords lists\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the English stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment analysis, we may want to keep all the negative words\n",
    "# i.e. we want to keep all the words below\n",
    "not_stop_words = [\n",
    "    \"aren't\",\n",
    "    'below',\n",
    "    'can',\n",
    "    \"couldn't\",\n",
    "    'did',\n",
    "    \"didn't\",\n",
    "    \"doesn't\",\n",
    "    \"don't\",\n",
    "    'few',\n",
    "    \"hadn't\",\n",
    "    \"hasn't\",\n",
    "    \"haven't\",\n",
    "    \"isn't\",\n",
    "    'more',\n",
    "    'most',\n",
    "    \"mustn't\",\n",
    "    \"needn't\",\n",
    "    'no',\n",
    "    'not',\n",
    "    'once',\n",
    "    'only',\n",
    "    'should',\n",
    "    \"should've\",\n",
    "    \"shouldn't\",\n",
    "    'so',\n",
    "    \"wasn't\",\n",
    "    \"weren't\",\n",
    "    \"won't\",\n",
    "    \"wouldn't\"\n",
    "]\n",
    "\n",
    "# remove the words above from the stop-words list\n",
    "for word in not_stop_words:\n",
    "    stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a set to identify punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = {char for char in string.punctuation}\n",
    "punctuation.add('...')  # add elipses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep words that aren't stop words and not punctuation\n",
    "word_list = [w for w in word_list if (w not in stop_words and w not in punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize one of the lemmatizers\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('eggs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Create a Representation\n",
    "\n",
    "I.e. convert the words to numbers somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\n",
    "    \"I do not like green eggs and ham.\",\n",
    "    \"I do not like them Sam-I-am.\",\n",
    "    \"I do not like them here or there.\",\n",
    "    \"I do not like them anywhere.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scikit-learn package has a bunch of realky good resources for ML in general, and they also have a TF-IDF function.\n",
    "\n",
    "See the [scikit-learn docs](https://scikit-learn.org/stable/index.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on `TfidfVectorizer` see [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the vecotrizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer` will do some text preprocessing by default. I believe it only tokenizes though, and doesn't remove stopwords or anything else. See the doc for more details though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the data\n",
    "X = vectorizer.fit_transform(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell us what the vocabulary is\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a sparse array by default, so we want a dense one to see it properly.\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify stop words.\n",
    "\n",
    "You can give `TfidfVectorizer` a list of stopwords, or tell it to use its default ones for a given language, which is what we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify an entire pre-processing routine.\n",
    "\n",
    "Sometimes you want more control over the pre-processing (specify your own stopwords, or lemmatization, or anything else).\n",
    "\n",
    "Specifying the `analyzer` attribute in `TfidfVectorizer` tells it not to do any pre-processing. Instead, the function you provide will be applied instead.\n",
    "\n",
    "That's what we'll do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, lemmatize, and remove stopwords\n",
    "def tok_lem_stop(sentence):\n",
    "\n",
    "    # tokenize the sentence\n",
    "    word_list = word_tokenize(sentence)\n",
    "\n",
    "    # convert everything to lower case\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "\n",
    "    # lemmatize the words in the sentence (only nouns)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "\n",
    "    # remove all stopwords and punctuation\n",
    "    word_list = [word for word in lemmatized_words if not (word in stop_words or word in punctuation)]\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell TfidfVectorizer to use our tok_lem_stop function for pre-processing instead of it's default\n",
    "vectorizer = TfidfVectorizer(analyzer=tok_lem_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Word Embeddings\n",
    "\n",
    "Converting words into vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Custom trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "word_list = [tok_lem_stop(sent) for sent in reviews[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is a topic modelling library for NLP and word2vec is a common word embedding algorithm\n",
    "# gensim has both pre-trained and custom-trainable word2vec options\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can train a custom word2vec embedding based on our dataset (or a part of it, in this case, although feel free to include the whole dataset!)\n",
    "model = Word2Vec(sentences=word_list, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the embedding (i.e. vector) for a given word\n",
    "model.wv['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the embedding (vector) for a sentence, you can average over all the embeddings \n",
    "# of the words in the sentence\n",
    "def get_average_embeddings(word_list, model):\n",
    "    n = len(word_list)\n",
    "    X = np.zeros((n, model.vector_size))\n",
    "    \n",
    "    for i, sent in enumerate(word_list):\n",
    "        x = np.zeros((len(sent), model.vector_size))\n",
    "        \n",
    "        for j, word in enumerate(sent):\n",
    "            x[j] = model.wv[word]\n",
    "\n",
    "        X[i] = x.mean(axis=0)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings = get_average_embeddings(word_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Pre-trained\n",
    "\n",
    "Gensim has a bunch of pre-trained word embedding models available including word2vec and GloVe models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "# Download the \"glove-twitter-25\" embeddings\n",
    "# Note that running the below statement may take a minute or two.\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "An out-of-the-box general purpose NLP library. \n",
    "\n",
    "For more info on SpaCy, see [their documentation](https://spacy.io/). I recommend doing the [**Spacy 101**](https://spacy.io/usage/spacy-101) part of their docs to familiarize yourself with it if you're just starting out. It is very easy to follow, and is a great introduction. Overall, SpaCy's docs are really good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download spacy's core English model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy's core English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply spacy's NLP model to the example sentence\n",
    "doc = nlp(\"This is an example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print some fancy things\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    print(f\"\\tPOS: {token.pos_}\")\n",
    "    print(f\"\\tLemma: {token.lemma_}\")\n",
    "    print(f\"\\tIs Stopword: {token.is_stop}\")    \n",
    "    print(f\"\\tIs Punctuation: {token.is_punct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between NLTK and SpaCy\n",
    "- tokenizer is slightly different\n",
    "- stopword lists are slightly different (SpaCy has more stopwords)\n",
    "- NLTK is more flexible, but SpaCy is easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing using NLTK\n",
    "word_tokenize(\"I do not like them Sam-I-am.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing using SpaCy\n",
    "[token for token in nlp(\"I do not like them Sam-I-am.\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
